{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Natebruh1/SentimentAnalysis/blob/main/SentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDWNRK4vIEip"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "import pickle\n",
        "import re\n",
        "import csv\n",
        "from enum import Enum\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import sklearn.metrics as metrics\n",
        "df=pd.read_csv(\"/content/drive/MyDrive/Content/IMDB Dataset.csv\",on_bad_lines='skip') #Movie Review Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VM5HCYnhE8rA"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#---NLTK DOWNLOADS---\n",
        "nltk.download('stopwords')\n",
        "#print(stopwords.words('english'))\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "\n",
        "#DATAFRAME WE ARE USING\n",
        "df=pd.read_csv(\"/content/drive/MyDrive/Content/IMDB Dataset.csv\",on_bad_lines='skip') #Movie Review Dataframe\n",
        "#print(df.head())\n",
        "#print(df.isnull().sum())\n",
        "\n",
        "\n",
        "\n",
        "#---ENUMS---\n",
        "class WordType(Enum):\n",
        "    verb=1\n",
        "    noun=2\n",
        "class DependencyParseTag(Enum):\n",
        "    verb=1\n",
        "    subject=2\n",
        "    obj=3\n",
        "\n",
        "\n",
        "\n",
        "#---Classes---\n",
        "\n",
        "#Sentences\n",
        "class SentenceObject:\n",
        "    sentenceData=\"\"\n",
        "    sentenceTokens=[] #Array of WordToken Objects\n",
        "    sentenceBigrams=[] #See how natural a sentence is\n",
        "    def __init__(self, sentence): #Contructor\n",
        "        self.sentenceData=sentence\n",
        "    def tokeniseSentence(self): #Convert our sentence Data to Tokens\n",
        "        for i in self.sentenceData.split():\n",
        "            self.sentenceTokens.append(WordToken(i))\n",
        "\n",
        "\n",
        "    def __getitem__(self, key): #Get the specified element from the key of the sentence\n",
        "        return self.sentenceTokens[key]\n",
        "\n",
        "\n",
        "    def removeStopWords(self):\n",
        "        for each in self.sentenceTokens:\n",
        "            tempStr=str(each) #Convert the word token to a string\n",
        "            if tempStr in stopwords.words('english'): #If the word is in our list of stop words\n",
        "\n",
        "                self.sentenceTokens.remove(each) #Remove it from the list\n",
        "    def __len__(self):\n",
        "        return len(self.sentenceTokens)\n",
        "    def __contains__(self,key):\n",
        "        return key in self.sentenceTokens\n",
        "\n",
        "\n",
        "#Word Tokens\n",
        "\n",
        "class WordToken:\n",
        "    tokenData=\"\"\n",
        "    def __init__(self,newToken): #Contructor\n",
        "        self.tokenData=newToken\n",
        "    def __str__(self): #Allows us to represent the object as a string (For printing)\n",
        "        return self.tokenData\n",
        "\n",
        "    def StemToken(self):\n",
        "        self.tokenData=nltk.stem.PorterStemmer().stem(self.tokenData) #Stem this token using PorterStemmer algorithm\n",
        "        #Algorithm imported from nltk library\n",
        "    def LemmatizeToken(self):\n",
        "        self.tokenData=nltk.stem.WordNetLemmatizer().lemmatize(self.tokenData) #Lemmatize this using the nltk library lemmatizer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Dependency Parse Tree\n",
        "# -RootToken\n",
        "# Subject[]\n",
        "# Object[]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    #Encode the sentiment of the dataframe\n",
        "    label=sklearn.preprocessing.LabelEncoder()\n",
        "    df[\"sentiment\"] = label.fit_transform(df[\"sentiment\"])\n",
        "    print(df.head())\n",
        "    print(df.shape)\n",
        "    independentData=df[\"review\"]\n",
        "    ##OOP APPROACH - used for testing but useful for determining more sentiments\n",
        "    \"\"\"sentenceObjArray=[]\n",
        "\n",
        "    for i in range(len(independentData)):\n",
        "        sentenceObjArray.append(SentenceObject(independentData[i]))\n",
        "        sentenceObjArray[i].tokeniseSentence()\n",
        "        for j in range(len(sentenceObjArray[i])):\n",
        "\n",
        "            sentenceObjArray[i][j].StemToken()\n",
        "            sentenceObjArray[i][j].LemmatizeToken()\n",
        "        sentenceObjArray[i].removeStopWords()\"\"\"\n",
        "\n",
        "    sentimentTable=df['sentiment']\n",
        "\n",
        "\n",
        "    ps=nltk.stem.PorterStemmer()\n",
        "    finalCorpus=[]\n",
        "    for i in range(len(independentData)):\n",
        "        print(i)\n",
        "        review=re.sub(\"[^a-zA-Z]\", \" \", independentData[i])\n",
        "        review=review.lower()\n",
        "        review=[ps.stem(word) for word in review if word not in set(stopwords.words(\"english\"))] #Stem the word if its not in the stopwords\n",
        "        review =\"\".join(review)\n",
        "        finalCorpus.append(review) # Add review to Final Corpus\n",
        "\n",
        "\n",
        "    pickle.dump(finalCorpus, open(\"/content/drive/MyDrive/Content/corpusData.pkl\",\"wb\"))\n",
        "\n",
        "    \"\"\"testSentence=\"The Quick Brown Fox Jumped Over The Corpora Lazy Dog\"\n",
        "\n",
        "    testSentObj=SentenceObject(testSentence)\n",
        "    #Test Stemming\n",
        "    testSentObj.tokeniseSentence()\n",
        "    testSentObj[4].StemToken()\n",
        "    print(testSentObj[4])\n",
        "\n",
        "    #Test Lemmatizing\n",
        "    testSentObj[8].LemmatizeToken()\n",
        "    print(testSentObj[8])\n",
        "\n",
        "    #Remove sentence stop words\n",
        "    testSentObj.removeStopWords()\n",
        "    print(testSentObj[0])\"\"\"\n",
        "main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "#Now we need to turn the text into vectors\n",
        "finalCorpus=pickle.load(open(\"/content/drive/MyDrive/Content/corpusData.pkl\",\"rb\"))\n",
        "cv=TfidfVectorizer(max_features=5000)\n",
        "#for i in range(500):\n",
        "#  print(finalCorpus[i])\n",
        "vectorizedMatrix=cv.fit_transform(finalCorpus).toarray()\n",
        "\n",
        "print(vectorizedMatrix.shape)\n",
        "label=sklearn.preprocessing.LabelEncoder()\n",
        "df[\"sentiment\"] = label.fit_transform(df[\"sentiment\"])\n",
        "sentimentTable=df['sentiment']\n",
        "vectorizedMatrix_trainingData, vectorizedMatrix_testingData, sentimentTable_trainingData, sentimentTable_testingData = train_test_split(vectorizedMatrix,sentimentTable, test_size=0.1, random_state=101)\n",
        "print(vectorizedMatrix_trainingData.shape, vectorizedMatrix_testingData.shape, sentimentTable_trainingData.shape, sentimentTable_testingData.shape)\n",
        "\n",
        "\n",
        "#Use Multinomial naive-bayes model\n",
        "mnb=MultinomialNB()\n",
        "#Train the model\n",
        "mnb.fit(vectorizedMatrix_trainingData,sentimentTable_trainingData)\n",
        "\n",
        "#Create Prediction\n",
        "pred=mnb.predict(vectorizedMatrix_testingData)\n",
        "print(metrics.accuracy_score(sentimentTable_testingData,pred))\n",
        "print(metrics.confusion_matrix(sentimentTable_testingData,pred))\n",
        "print(metrics.classification_report(sentimentTable_testingData,pred))\n",
        "pd.DataFrame(np.c_[sentimentTable_testingData,pred],columns=[\"Actual\",\"Predicted\"])\n",
        "#Save Prediction\n",
        "pickle.dump(cv, open(\"/content/drive/MyDrive/Content/count-vectorizer.pkl\",\"wb\"))\n",
        "pickle.dump(mnb, open(\"/content/drive/MyDrive/Content/multinomial-naive-bayes.pkl\",\"wb\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmucib9pdiaU",
        "outputId": "e4fccd1e-001f-4c79-a080-611a31a089fc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 5000)\n",
            "(45000, 5000) (5000, 5000) (45000,) (5000,)\n",
            "0.8418\n",
            "[[2061  325]\n",
            " [ 466 2148]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.86      0.84      2386\n",
            "           1       0.87      0.82      0.84      2614\n",
            "\n",
            "    accuracy                           0.84      5000\n",
            "   macro avg       0.84      0.84      0.84      5000\n",
            "weighted avg       0.84      0.84      0.84      5000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_cv=pickle.load(open(\"/content/drive/MyDrive/Content/count-vectorizer.pkl\",\"rb\"))\n",
        "loaded_model=pickle.load(open(\"/content/drive/MyDrive/Content/multinomial-naive-bayes.pkl\",\"rb\"))\n",
        "\n",
        "def test_sentence(inp):\n",
        "  sen=loaded_cv.transform([inp]).toarray()\n",
        "  res = loaded_model.predict(sen)[0]\n",
        "  if res==1:\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "print(test_sentence(\"Good movie great job\"))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPXWMFYTy5p6",
        "outputId": "f8d04e95-96c1-4067-9d43-6df512a77ee4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMYSHCG86ZdjsSblIRLO8St",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}